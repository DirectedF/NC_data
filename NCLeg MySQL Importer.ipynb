{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55644d32-84f2-45ad-bf5e-e64183362ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. NC Bills by Last Action pulled in. RSS feed.\n",
      "02. NC Legistlative site news pulled in. RSS feed.\n",
      "03. All Committees -active and nonactive- pulled. JSON feed.\n",
      "04. List of All Filed House Bills downloaded. RSS feed.\n",
      "     Changes to List of All Filed House Bills have been saved to the table: filed_bills_house_2023_changelog_12302023\n",
      "05. List of All Filed Senate Bills downloaded. RSS feed.\n",
      "     Changes to List of All Filed Senate Bills have been saved to the table: filed_bills_senate_2023_changelog_12302023\n",
      "06. NC Legislative Documents downloaded. RSS feed.\n",
      "07. Table not found in the webpage\n",
      "Either 'Bill_ID' column is missing or one of the DataFrames is empty. Skipping merge.\n",
      "07. Scraped modifications to current laws. Page scrape.\n",
      "08. Scraped current House members. Page scrape.\n",
      "09. Scraped current Senate members. Page scrape.\n",
      "10. Scraped current House and Senate Leadership Titles for current session. Page scrape.\n",
      "11. Scraped current House and Senate Contact Info for current session. Page scrape.\n",
      " \n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy.exc import IntegrityError, OperationalError\n",
    "from sqlalchemy.orm import declarative_base  # Updated import for declarative_base\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, text, MetaData, inspect\n",
    "from sqlalchemy.sql import text  # Import the text function for raw SQL execution\n",
    "from sqlalchemy.dialects.mysql import VARCHAR\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# RSS feed URL for BILLS (with Last Action)\n",
    "rss_url = \"https://www.ncleg.gov/Legislation/Bills/LastActionByYear/2023/All/RSS\"\n",
    "\n",
    "# Parse the RSS feed\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "# Extracting details into lists\n",
    "bill_ids = []\n",
    "titles = []\n",
    "descriptions = []\n",
    "links = []\n",
    "published_dates = []\n",
    "\n",
    "for entry in feed.entries:\n",
    "    link = entry.link\n",
    "    matches = re.findall(r'/(\\d+)/([A-Z]\\d+)', link)\n",
    "    unique_id = \"_\".join(matches[0]) if matches else str(uuid.uuid4())\n",
    "    \n",
    "    bill_ids.append(unique_id)\n",
    "    titles.append(entry.title)\n",
    "    descriptions.append(entry.description)\n",
    "    links.append(link)\n",
    "    \n",
    "# Example of a more complete date format, adjust as needed\n",
    "date_format = \"%a, %d %b %Y\"  # Adjusted format without time\n",
    "\n",
    "for entry in feed.entries:\n",
    "    # ... [other code] ...\n",
    "\n",
    "    # Extract date string without time and timezone\n",
    "    date_string = entry.published.split(' ', 4)[:4]\n",
    "    date_string = ' '.join(date_string)\n",
    "\n",
    "    try:\n",
    "        dt = datetime.strptime(date_string, date_format)\n",
    "        published_dates.append(dt.strftime('%Y-%m-%d'))  # Format suitable for MySQL DATE\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing date '{date_string}': {e}\")\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Bill_ID\": bill_ids,\n",
    "    \"Bill_Title\": titles,\n",
    "    \"Bill_Description\": descriptions,\n",
    "    \"Bill_Link\": links,\n",
    "    \"Bill_Published_Date\": published_dates\n",
    "})\n",
    "\n",
    "# Connect to MySQL database\n",
    "DATABASE_URI = \"mysql+pymysql://kurt712:Gwyn-072022!@localhost/NCLeg\"\n",
    "engine = create_engine(DATABASE_URI, echo=False)\n",
    "\n",
    "# Clear existing data and insert new data\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(\"DELETE FROM bills;\")  # Clear the table\n",
    "    df.to_sql('bills', engine, if_exists='append', index=False, method='multi')  # Insert new data\n",
    "    print(\"01. NC Bills by Last Action pulled in. RSS feed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Define the RSS feed URL for NEWS\n",
    "# NEWS\n",
    "rss_url = \"https://www.ncleg.gov/News/RSS\"\n",
    "\n",
    "# Parse the RSS feed\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "# Extracting details into lists\n",
    "titles = []\n",
    "links = []\n",
    "published_dates = []\n",
    "descriptions = []\n",
    "\n",
    "for entry in feed.entries:\n",
    "    titles.append(entry.title)\n",
    "    links.append(entry.link)\n",
    "    \n",
    "    # Convert the published date into a MySQL-friendly format\n",
    "    date_format = \"%a, %d %b %Y %H:%M:%S\"\n",
    "    # Removing timezone info from the string\n",
    "    date_string = entry.published.rsplit(' ', 1)[0]\n",
    "    dt = datetime.strptime(date_string, date_format)\n",
    "\n",
    "    # If you know the timezone of the entry.published and want to make it timezone-aware, you can use:\n",
    "    # dt = dt.replace(tzinfo=pytz.timezone('US/Eastern'))  # Assuming EST is US/Eastern\n",
    "    # dt = dt.astimezone(pytz.utc)  # Convert to UTC\n",
    "    published_dates.append(dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    descriptions.append(entry.description)\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Title\": titles,\n",
    "    \"Link\": links,\n",
    "    \"Published Date\": published_dates,\n",
    "    \"Description\": descriptions\n",
    "})\n",
    "\n",
    "# Connect to MySQL database\n",
    "connection = pymysql.connect(host='localhost',\n",
    "                             user='kurt712',\n",
    "                             password='Gwyn-072022!',\n",
    "                             database='NCLeg', \n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "# Create a new table (if it doesn't exist)\n",
    "table_creation_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS news_2023 (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    Title TEXT,\n",
    "    Link TEXT,\n",
    "    Published_Date DATETIME,\n",
    "    Description TEXT\n",
    ")\n",
    "\"\"\"\n",
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(table_creation_query)\n",
    "    connection.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "for index, row in df.iterrows():\n",
    "    with connection.cursor() as cursor:\n",
    "        # Check if entry already exists\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM news_2023 WHERE Link=%s\", (row[\"Link\"],))\n",
    "        if cursor.fetchone()[\"COUNT(*)\"] == 0:\n",
    "            # If not, insert\n",
    "            insert_query = \"\"\"\n",
    "            INSERT INTO news_2023 (Title, Link, Published_Date, Description)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            cursor.execute(insert_query, (row[\"Title\"], row[\"Link\"], row[\"Published Date\"], row[\"Description\"]))\n",
    "            connection.commit()\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n",
    "\n",
    "print(\"02. NC Legistlative site news pulled in. RSS feed.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Define the URL for JSON AllActiveCommittees\n",
    "\n",
    "url = \"https://webservices.ncleg.gov/AllActiveCommittees\"\n",
    "\n",
    "# Fetch the JSON data from the URL\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Extracting details into lists\n",
    "committee_ids = []\n",
    "chamber_codes = []\n",
    "session_codes = []\n",
    "committee_names = []\n",
    "committee_names_chamber = []\n",
    "doc_site_ids = []\n",
    "select_committee_yns = []\n",
    "nonstanding_committee__yns = []\n",
    "joint_select_committee__yns = []\n",
    "\n",
    "for item in data:\n",
    "    committee_ids.append(item['nCommitteeID'])\n",
    "    chamber_codes.append(item['sChamberCode'])\n",
    "    session_codes.append(item['sSessionCode'])\n",
    "    committee_names.append(item['sCommitteeName'])\n",
    "    committee_names_chamber.append(item['sCommitteeNameWithChamber'])\n",
    "    doc_site_ids.append(item['nDocSiteID'])\n",
    "    select_committee_yns.append(item['bSelectCommittee'])\n",
    "    nonstanding_committee__yns.append(item['bNonStandingCommittee'])\n",
    "    joint_select_committee__yns.append(item['bJointSelectCommittee'])\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Committee_ID\": committee_ids,\n",
    "    \"Chamber_Code\": chamber_codes,\n",
    "    \"Session_Code\": session_codes,\n",
    "    \"Committee_Name\": committee_names,\n",
    "    \"Committee_Name_with_Chamber\": committee_names_chamber,\n",
    "    \"DocSite_ID\": doc_site_ids,\n",
    "    \"Select_Committee\": select_committee_yns,\n",
    "    \"Non_Standing_Committee\": nonstanding_committee__yns,\n",
    "    \"Joint_Select_Committee\": joint_select_committee__yns\n",
    "})\n",
    "\n",
    "# Connect to MySQL database\n",
    "connection = pymysql.connect(host='localhost',\n",
    "                             user='kurt712',\n",
    "                             password='Gwyn-072022!',\n",
    "                             database='NCLeg',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "# Create a new table (if it doesn't exist)\n",
    "table_creation_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS committees_2023 (\n",
    "    Committee_ID INT PRIMARY KEY,\n",
    "    Chamber_Code TEXT,\n",
    "    Session_Code TEXT,\n",
    "    Committee_Name TEXT,\n",
    "    Committee_Name_with_Chamber TEXT,\n",
    "    DocSite_ID INT,\n",
    "    Select_Committee BOOL,\n",
    "    Non_Standing_Committee BOOL,\n",
    "    Joint_Select_Committee BOOL\n",
    ")\n",
    "\"\"\"\n",
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(table_creation_query)\n",
    "    connection.commit()\n",
    "\n",
    "# fix NULL in SQL\n",
    "df['DocSite_ID'] = df['DocSite_ID'].fillna(0).astype(int)  # replace nan with 0 and ensure the column is of type int\n",
    "\n",
    "# Insert data into the table\n",
    "for index, row in df.iterrows():\n",
    "    with connection.cursor() as cursor:\n",
    "        # Check if entry already exists\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM committees_2023 WHERE Committee_ID=%s\", (row[\"Committee_ID\"],))\n",
    "        if cursor.fetchone()[\"COUNT(*)\"] == 0:\n",
    "            # If not, insert\n",
    "            insert_query = \"\"\"\n",
    "            INSERT INTO committees_2023 (Committee_ID, Chamber_Code, Session_Code, Committee_Name, Committee_Name_with_Chamber, DocSite_ID, Select_Committee, Non_Standing_Committee, Joint_Select_Committee)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            cursor.execute(insert_query, (row[\"Committee_ID\"], row[\"Chamber_Code\"], row[\"Session_Code\"], row[\"Committee_Name\"], row[\"Committee_Name_with_Chamber\"], row[\"DocSite_ID\"], row[\"Select_Committee\"], row[\"Non_Standing_Committee\"], row[\"Joint_Select_Committee\"]))\n",
    "            connection.commit()\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n",
    "\n",
    "\n",
    "print(\"03. All Committees -active and nonactive- pulled. JSON feed.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Define the XML feed URL for Filed HOUSE Bills\n",
    "# \n",
    "\n",
    "rss_url = \"https://www.ncleg.gov/Legislation/Bills/FiledBillsFeed/2023/H\"\n",
    "\n",
    "# Parse the RSS feed\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "# Extracting details into lists\n",
    "descriptions = []\n",
    "bill_ids = []\n",
    "bills = []\n",
    "links = []\n",
    "fileddates = []\n",
    "shorttitles = []\n",
    "primarysponsors_es = []\n",
    "longtitles = []\n",
    "billtypes = []\n",
    "localpublics = []\n",
    "appropriations_es = []\n",
    "fiscalimpacts = []\n",
    "constitutions = []\n",
    "studies_es = []\n",
    "rollcalls = []\n",
    "appropriation_es = []\n",
    "\n",
    "for entry in feed.entries:\n",
    "    descriptions.append(entry.description)\n",
    "    matches = re.search(r'/(\\d{4})/([A-Z]+)-(\\d+)-', entry.link)  # Use entry.link instead of just link\n",
    "\n",
    "    if matches:\n",
    "        year_id = matches.group(1)\n",
    "        letter_id = matches.group(2)\n",
    "        bill_number_id = matches.group(3)\n",
    "        unique_id = f\"{year_id}_{letter_id}{bill_number_id}\" if matches else str(uuid.uuid4())\n",
    "    else:\n",
    "        unique_id = str(uuid.uuid4())\n",
    "    \n",
    "    bill_ids.append(unique_id)\n",
    "    bills.append(entry.bill)\n",
    "    links.append(entry.link)\n",
    "   \n",
    "    # Convert the fileddate date into a MySQL-friendly format\n",
    "    date_format = \"%m/%d/%Y\"\n",
    "    # Removing timezone info from the string\n",
    "    date_string = entry.fileddate.rsplit(' ', 1)[0]\n",
    "    dt = datetime.strptime(date_string, date_format)\n",
    "\n",
    "    # If you know the timezone of the entry.fileddate and want to make it timezone-aware, you can use:\n",
    "    # dt = dt.replace(tzinfo=pytz.timezone('US/Eastern'))  # Assuming EST is US/Eastern\n",
    "    # dt = dt.astimezone(pytz.utc)  # Convert to UTC\n",
    "    \n",
    "    fileddates.append(dt.strftime('%m-%d-%Y'))\n",
    "    \n",
    "    shorttitles.append(entry.shorttitle)\n",
    "    primarysponsors_es.append(entry.primarysponsors)\n",
    "    longtitles.append(entry.longtitle)\n",
    "    billtypes.append(entry.billtype)\n",
    "    localpublics.append(entry.localpublic)\n",
    "    appropriations_es.append(entry.appropriations)\n",
    "    fiscalimpacts.append(entry.fiscalimpact)\n",
    "    constitutions.append(entry.constitution)\n",
    "    studies_es.append(entry.studies)\n",
    "    rollcalls.append(entry.rollcall)\n",
    "    appropriation_es.append(entry.appropriation)\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Bill_ID\": bill_ids,\n",
    "    \"Bill\": bills,\n",
    "    \"Link\": links,\n",
    "    \"File_Date\": fileddates,\n",
    "    \"Title_Short\": shorttitles,\n",
    "    \"Primary_Sponsors\": primarysponsors_es,\n",
    "    \"Title_Long\": longtitles,\n",
    "    \"Bill_Type\": billtypes,\n",
    "    \"Local-Public\": localpublics,\n",
    "    \"Appropriations\": appropriations_es,\n",
    "    \"Fiscal_Impacts\": fiscalimpacts,\n",
    "    \"Constitutions\": constitutions,\n",
    "    \"Studies\": studies_es,\n",
    "    \"Roll_Call\": rollcalls,\n",
    "    \"Appropriation\": appropriation_es\n",
    "    \n",
    "})\n",
    "\n",
    "# Connect to MySQL database using SQLAlchemy\n",
    "DATABASE_URL = \"mysql+pymysql://kurt712:Gwyn-072022!@localhost/NCLeg\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Check if the table exists\n",
    "if inspector.has_table('ncleg.filed_bills_house_2023'):\n",
    "    # Load data from SQL table into another dataframe\n",
    "    query = \"SELECT * FROM ncleg.filed_bills_house_2023\"\n",
    "    existing_data = pd.read_sql(query, engine)\n",
    "else:\n",
    "    # If the table doesn't exist, create it and set existing_data to an empty dataframe with the same columns as df\n",
    "    df.to_sql('ncleg.filed_bills_house_2023', engine, if_exists='append', index=False)\n",
    "    existing_data = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# Load data from SQL table into another dataframe\n",
    "query = \"SELECT * FROM ncleg.filed_bills_house_2023\"\n",
    "existing_data = pd.read_sql(query, engine)\n",
    "\n",
    "# Merge the new data with the existing data to identify changes\n",
    "merged_data = pd.merge(df, existing_data, on='Bill_ID', how='outer', indicator=True)\n",
    "\n",
    "# Filter out rows that haven't changed\n",
    "changes = merged_data[merged_data['_merge'] != 'both']\n",
    "\n",
    "# Generate a table name with today's date in MMDDYYYY format\n",
    "today = datetime.today().strftime('%m%d%Y')\n",
    "table_name = f\"filed_bills_house_2023_changelog_{today}\"\n",
    "\n",
    "# Create a new table with the generated name in the 'ncleg_changelogs' schema and insert changes\n",
    "changes.to_sql(table_name, engine, schema='ncleg_changelogs', if_exists='replace', index=False)\n",
    "\n",
    "# Rename columns for the changes dataframe to match the table schema\n",
    "changes.columns = [col + \"_old\" if col in df.columns and col != \"_merge\" else col for col in changes.columns]\n",
    "changes.columns = [col + \"_new\" if col in existing_data.columns and col != \"_merge\" else col for col in changes.columns]\n",
    "\n",
    "# Generate a table name with today's date in MMDDYYYY format\n",
    "today = datetime.today().strftime('%m%d%Y')\n",
    "table_name = f\"filed_bills_house_2023_changelog_{today}\"\n",
    "\n",
    "# Create a new table with the generated name in the 'ncleg_changelogs' schema and insert changes\n",
    "changes.to_sql(table_name, engine, schema='ncleg_changelogs', if_exists='replace', index=False)\n",
    "\n",
    "print(\"04. List of All Filed House Bills downloaded. RSS feed.\")\n",
    "print(f\"     Changes to List of All Filed House Bills have been saved to the table: {table_name}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Define the XML feed URL for Filed SENATE Bills\n",
    "# \n",
    "\n",
    "rss_url = \"https://www.ncleg.gov/Legislation/Bills/FiledBillsFeed/2023/S\"\n",
    "\n",
    "# Parse the RSS feed\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "# Extracting details into lists\n",
    "descriptions = []\n",
    "bill_ids = []\n",
    "bills = []\n",
    "links = []\n",
    "fileddates = []\n",
    "shorttitles = []\n",
    "primarysponsors_es = []\n",
    "longtitles = []\n",
    "billtypes = []\n",
    "localpublics = []\n",
    "appropriations_es = []\n",
    "fiscalimpacts = []\n",
    "constitutions = []\n",
    "studies_es = []\n",
    "rollcalls = []\n",
    "appropriation_es = []\n",
    "\n",
    "for entry in feed.entries:\n",
    "    descriptions.append(entry.description)\n",
    "    matches = re.search(r'/(\\d{4})/([A-Z]+)-(\\d+)-', entry.link)  # Use entry.link instead of just link\n",
    "\n",
    "    if matches:\n",
    "        year_id = matches.group(1)\n",
    "        letter_id = matches.group(2)\n",
    "        bill_number_id = matches.group(3)\n",
    "        unique_id = f\"{year_id}_{letter_id}{bill_number_id}\" if matches else str(uuid.uuid4())\n",
    "    else:\n",
    "        unique_id = str(uuid.uuid4())\n",
    "    \n",
    "    bill_ids.append(unique_id)\n",
    "    bills.append(entry.bill)\n",
    "    links.append(entry.link)\n",
    "   \n",
    "    # Convert the fileddate date into a MySQL-friendly format\n",
    "    date_format = \"%m/%d/%Y\"\n",
    "    # Removing timezone info from the string\n",
    "    date_string = entry.fileddate.rsplit(' ', 1)[0]\n",
    "    dt = datetime.strptime(date_string, date_format)\n",
    "\n",
    "    # If you know the timezone of the entry.fileddate and want to make it timezone-aware, you can use:\n",
    "    # dt = dt.replace(tzinfo=pytz.timezone('US/Eastern'))  # Assuming EST is US/Eastern\n",
    "    # dt = dt.astimezone(pytz.utc)  # Convert to UTC\n",
    "    \n",
    "    fileddates.append(dt.strftime('%m-%d-%Y'))\n",
    "    \n",
    "    shorttitles.append(entry.shorttitle)\n",
    "    primarysponsors_es.append(entry.primarysponsors)\n",
    "    longtitles.append(entry.longtitle)\n",
    "    billtypes.append(entry.billtype)\n",
    "    localpublics.append(entry.localpublic)\n",
    "    appropriations_es.append(entry.appropriations)\n",
    "    fiscalimpacts.append(entry.fiscalimpact)\n",
    "    constitutions.append(entry.constitution)\n",
    "    studies_es.append(entry.studies)\n",
    "    rollcalls.append(entry.rollcall)\n",
    "    appropriation_es.append(entry.appropriation)\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Bill_ID\": bill_ids,\n",
    "    \"Bill\": bills,\n",
    "    \"Link\": links,\n",
    "    \"File_Date\": fileddates,\n",
    "    \"Title_Short\": shorttitles,\n",
    "    \"Primary_Sponsors\": primarysponsors_es,\n",
    "    \"Title_Long\": longtitles,\n",
    "    \"Bill_Type\": billtypes,\n",
    "    \"Local-Public\": localpublics,\n",
    "    \"Appropriations\": appropriations_es,\n",
    "    \"Fiscal_Impacts\": fiscalimpacts,\n",
    "    \"Constitutions\": constitutions,\n",
    "    \"Studies\": studies_es,\n",
    "    \"Roll_Call\": rollcalls,\n",
    "    \"Appropriation\": appropriation_es\n",
    "\n",
    "})\n",
    "\n",
    "# Connect to MySQL database using SQLAlchemy\n",
    "DATABASE_URL = \"mysql+pymysql://kurt712:Gwyn-072022!@localhost/NCLeg\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Check if the table exists\n",
    "if inspector.has_table('ncleg.filed_bills_senate_2023'):\n",
    "    # Load data from SQL table into another dataframe\n",
    "    query = \"SELECT * FROM ncleg.filed_bills_senate_2023\"\n",
    "    existing_data = pd.read_sql(query, engine)\n",
    "else:\n",
    "    # If the table doesn't exist, create it and set existing_data to an empty dataframe with the same columns as df\n",
    "    df.to_sql('ncleg.filed_bills_senate_2023', engine, if_exists='append', index=False)\n",
    "    existing_data = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "# Load data from SQL table into another dataframe\n",
    "query = \"SELECT * FROM ncleg.filed_bills_senate_2023\"\n",
    "existing_data = pd.read_sql(query, engine)\n",
    "\n",
    "# Merge the new data with the existing data to identify changes\n",
    "merged_data = pd.merge(df, existing_data, on='Bill_ID', how='outer', indicator=True)\n",
    "\n",
    "# Filter out rows that haven't changed\n",
    "changes = merged_data[merged_data['_merge'] != 'both']\n",
    "\n",
    "# Generate a table name with today's date in MMDDYYYY format\n",
    "today = datetime.today().strftime('%m%d%Y')\n",
    "table_name = f\"filed_bills_senate_2023_changelog_{today}\"\n",
    "\n",
    "# Create a new table with the generated name in the 'ncleg_changelogs' schema and insert changes\n",
    "changes.to_sql(table_name, engine, schema='ncleg_changelogs', if_exists='replace', index=False)\n",
    "\n",
    "# Rename columns for the changes dataframe to match the table schema\n",
    "changes.columns = [col + \"_old\" if col in df.columns and col != \"_merge\" else col for col in changes.columns]\n",
    "changes.columns = [col + \"_new\" if col in existing_data.columns and col != \"_merge\" else col for col in changes.columns]\n",
    "\n",
    "# Generate a table name with today's date in MMDDYYYY format\n",
    "today = datetime.today().strftime('%m%d%Y')\n",
    "table_name = f\"filed_bills_senate_2023_changelog_{today}\"\n",
    "\n",
    "# Create a new table with the generated name in the 'ncleg_changelogs' schema and insert changes\n",
    "changes.to_sql(table_name, engine, schema='ncleg_changelogs', if_exists='replace', index=False)\n",
    "\n",
    "print(\"05. List of All Filed Senate Bills downloaded. RSS feed.\")\n",
    "print(f\"     Changes to List of All Filed Senate Bills have been saved to the table: {table_name}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# the RSS feed URL for Legislative Documents\n",
    "# DOCS\n",
    "rss_url = \"https://www.ncleg.gov/Documents/RSS/1/12152\"\n",
    "\n",
    "# Parse the RSS feed\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "# Extracting details into lists\n",
    "guids = []\n",
    "links = []\n",
    "titles = []\n",
    "descriptions = []\n",
    "pubDates = []\n",
    "\n",
    "for entry in feed.entries:\n",
    "    guids.append(entry.guid)\n",
    "    links.append(entry.link)\n",
    "    titles.append(entry.title)\n",
    "    descriptions.append(entry.description) \n",
    "    \n",
    "    # Convert the pubDate into a MySQL-friendly format\n",
    "    date_format = \"%a, %d %b %Y %H:%M:%S\"\n",
    "    # Removing timezone info from the string\n",
    "    date_string = entry.published.rsplit(' ', 1)[0]\n",
    "    dt = datetime.strptime(date_string, date_format)\n",
    "\n",
    "    # If you know the timezone of the entry.pubDates and want to make it timezone-aware, you can use:\n",
    "    # dt = dt.replace(tzinfo=pytz.timezone('US/Eastern'))  # Assuming EST is US/Eastern\n",
    "    # dt = dt.astimezone(pytz.utc)  # Convert to UTC\n",
    "    pubDates.append(dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Doc_GU_ID\": guids,\n",
    "    \"Doc_Link\": links,\n",
    "    \"Doc_Title\": titles,\n",
    "    \"Doc_Description\": descriptions,\n",
    "    \"Doc_Pubication_Date\": pubDates\n",
    "})\n",
    "\n",
    "# ... [Your existing code for parsing the RSS feed and creating the DataFrame] ...\n",
    "\n",
    "# Connect to MySQL database\n",
    "connection = pymysql.connect(host='localhost',\n",
    "                             user='kurt712',\n",
    "                             password='Gwyn-072022!',\n",
    "                             database='NCLeg', \n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "# Create a new table (if it doesn't exist)\n",
    "table_creation_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS docs_2023 (\n",
    "    Doc_GU_ID VARCHAR(25) PRIMARY KEY,\n",
    "    Doc_Link TEXT,\n",
    "    Doc_Title TEXT,\n",
    "    Doc_Description TEXT,\n",
    "    Doc_Pubication_Date DATETIME\n",
    ")\n",
    "\"\"\"\n",
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(table_creation_query)\n",
    "\n",
    "# Insert data into the table\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO docs_2023 (Doc_GU_ID, Doc_Link, Doc_Title, Doc_Description, Doc_Pubication_Date)\n",
    "    VALUES (%s, %s, %s, %s, %s)\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "    Doc_Link = VALUES(Doc_Link),\n",
    "    Doc_Title = VALUES(Doc_Title),\n",
    "    Doc_Description = VALUES(Doc_Description),\n",
    "    Doc_Pubication_Date = VALUES(Doc_Pubication_Date)\n",
    "\"\"\"\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    for index, row in df.iterrows():\n",
    "        cursor.execute(insert_query, (row[\"Doc_GU_ID\"], row[\"Doc_Link\"], row[\"Doc_Title\"], row[\"Doc_Description\"], row[\"Doc_Pubication_Date\"]))\n",
    "    connection.commit()\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n",
    "\n",
    "print(\"06. NC Legislative Documents downloaded. RSS feed.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Webpage data scrape for Modifications to Laws in the current session.\n",
    "#\n",
    "\n",
    "# Scrape the website\n",
    "url = \"https://www.ncleg.gov/Laws/Modifications\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "table = soup.find(\"table\", attrs={\"class\": \"display responsive table table-striped table-bordered\"})\n",
    "\n",
    "laws = []\n",
    "if table:\n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) >= 7:  # Ensure there are enough columns\n",
    "            law = {\n",
    "                \"Citation\": cols[0].get_text(),\n",
    "                \"Type\": cols[1].get_text(),\n",
    "                \"Session_Law\": cols[2].get_text(),\n",
    "                \"Bill_ID\": \"2023_\" + cols[3].get_text(),\n",
    "                \"Bill\": cols[3].get_text(),\n",
    "                \"Bill_Section\": cols[4].get_text(),\n",
    "                \"Effective_Date\": cols[5].get_text(),\n",
    "                \"Status_of_Bill\": cols[6].get_text() if cols[6].get_text() != \"\" else None\n",
    "            }\n",
    "            laws.append(law)\n",
    "else:\n",
    "    print(\"07. Table not found in the webpage\")\n",
    "\n",
    "# Convert the laws list to a pandas DataFrame\n",
    "df = pd.DataFrame(laws)\n",
    "\n",
    "# Connect to MySQL database using SQLAlchemy\n",
    "DATABASE_URL = \"mysql+pymysql://kurt712:Gwyn-072022!@localhost/NCLeg\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Check if the laws_modified table exists and load data from it\n",
    "if inspector.has_table('ncleg.laws_modified'):\n",
    "    existing_data = pd.read_sql(\"SELECT * FROM ncleg.laws_modified\", engine)\n",
    "else:\n",
    "    existing_data = pd.DataFrame()\n",
    "\n",
    "# Check if 'Bill_ID' column exists in both dataframes and neither is empty\n",
    "if 'Bill_ID' in df.columns and 'Bill_ID' in existing_data.columns and not df.empty and not existing_data.empty:\n",
    "    # Ensure Bill_ID is treated as a string\n",
    "    df['Bill_ID'] = df['Bill_ID'].astype(str)\n",
    "\n",
    "    # Merge the new data with the existing data to identify changes\n",
    "    merged_data = pd.merge(df, existing_data, on='Bill_ID', how='outer', indicator=True)\n",
    "\n",
    "    # Filter out rows that haven't changed\n",
    "    changes = merged_data[merged_data['_merge'] != 'both']\n",
    "\n",
    "    # ... [Rest of your code for processing changes and updating SQL table] ...\n",
    "else:\n",
    "    print(\"Either 'Bill_ID' column is missing or one of the DataFrames is empty. Skipping merge.\")\n",
    "\n",
    "print(\"07. Scraped modifications to current laws. Page scrape.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Webpage data scrape for House Member names for current session.\n",
    "#\n",
    "\n",
    "# Scrape the website for member data\n",
    "url = \"https://www.ncleg.gov/Members/MemberTable/H\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "table = soup.find(\"table\", attrs={\"class\": \"display responsive table table-striped table-bordered\"})\n",
    "\n",
    "members_house = []\n",
    "if table:\n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        cols = row.find_all(\"td\")\n",
    "        # ... [Your existing code to process each row and append to members_house] ...\n",
    "\n",
    "# Convert the members list to a pandas DataFrame\n",
    "df_members = pd.DataFrame(members_house)\n",
    "\n",
    "# Connect to MySQL database using SQLAlchemy\n",
    "DATABASE_URL = \"mysql+pymysql://kurt712:Gwyn-072022!@localhost/NCLeg\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# This will create the table if it doesn't exist\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "# Function to check if a member already exists in the table\n",
    "def member_exists(member_id, connection):\n",
    "    query = text(\"SELECT COUNT(*) FROM members_house WHERE `Member_ID` = :member_id\")\n",
    "    result = connection.execute(query, {'member_id': member_id.strip()})\n",
    "    return result.scalar() > 0\n",
    "\n",
    "# Insert data into the members_house table\n",
    "with engine.begin() as connection:\n",
    "    if inspector.has_table('members_house'):\n",
    "        for index, row in df_members.iterrows():\n",
    "            try:\n",
    "                if not member_exists(row['Member_ID'], connection):\n",
    "                    row_df = pd.DataFrame([row])\n",
    "                    row_df.to_sql('members_house', con=engine, if_exists='append', index=False)\n",
    "            except IntegrityError as e:\n",
    "                print(f\"IntegrityError encountered: {e.orig.args}\")\n",
    "                print(f\"Skipping duplicate entry for member ID: {row['Member_ID']}\")\n",
    "    else:\n",
    "        print(\"Members table not found.\")\n",
    "\n",
    "print(\"08. Scraped current House members. Page scrape.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Webpage data scrape for Senate Member names for current session.\n",
    "#\n",
    "\n",
    "# Scrape the website\n",
    "url = \"https://www.ncleg.gov/Members/MemberTable/S\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "table = soup.find(\"table\", attrs={\"class\": \"display responsive table table-striped table-bordered\"})\n",
    "\n",
    "Base = declarative_base()\n",
    "    \n",
    "class MembersSenate(Base):\n",
    "    __tablename__ = 'members_senate'\n",
    "\n",
    "    Member_ID = Column(String(100), primary_key=True)\n",
    "    Party = Column(String(10))\n",
    "    District = Column(String(10))\n",
    "    Member_Last_Name = Column(String(50))\n",
    "    Member_Name = Column(String(125))\n",
    "    Counties_Represented = Column(Text)\n",
    "    District_URL = Column(String(255))\n",
    "    Member_URL = Column(String(255))\n",
    "\n",
    "members_senate = []\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    cols = row.find_all(\"td\")\n",
    "    \n",
    "    district_url = cols[2].find('a')['href'] if cols[2].find('a') else None\n",
    "    member_url = cols[4].find('a')['href'] if cols[4].find('a') else None\n",
    "\n",
    "    # Extracting the unique identifier\n",
    "    if member_url:\n",
    "        matches = re.findall(r'/(.)/(\\d+)', member_url)\n",
    "        if matches:\n",
    "            unique_id = matches[0][0] + \"M\" + matches[0][1]\n",
    "        else:\n",
    "            print(f\"No match for {member_url}\")  # This will help in debugging\n",
    "            unique_id = str(uuid.uuid4())  # Generate a UUID if no match is found\n",
    "    else:\n",
    "        unique_id = str(uuid.uuid4())  # Generate a UUID if district_url is None\n",
    "\n",
    "    member_senate = {\n",
    "        \"Party\": cols[0].get_text(),\n",
    "        \"District\": cols[2].get_text(),\n",
    "        \"Member_Last_Name\": cols[3].get_text(),\n",
    "        \"Member_Name\": cols[4].get_text(),\n",
    "        \"Counties_Represented\": cols[5].get_text() if cols[5].get_text() != \"\" else None,\n",
    "        \"District_URL\": district_url,\n",
    "        \"Member_URL\": member_url,\n",
    "        \"Member_ID\": unique_id   # Adding the unique identifier\n",
    "    }\n",
    "    members_senate.append(member_senate)\n",
    "    \n",
    "# Convert the laws list to a pandas DataFrame\n",
    "df = pd.DataFrame(members_senate)\n",
    "df.rename(columns={\"Member_Last_Name\": \"Member_Last_Name\"}, inplace=True)\n",
    "\n",
    "# Create a SQLAlchemy connection\n",
    "DATABASE_URI = \"mysql+pymysql://kurt712:Gwyn-072022!@localhost/NCLeg\"\n",
    "engine = create_engine(DATABASE_URI, echo=False)\n",
    "\n",
    "# This will create the table if it doesn't exist\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "# Create a function to check if a member already exists in the table\n",
    "def member_exists(member_id, connection):\n",
    "    query = text(\"SELECT COUNT(*) FROM members_senate WHERE `Member_ID` = :member_id\")\n",
    "    result = connection.execute(query, {'member_id': member_id.strip()})\n",
    "    return result.scalar() > 0\n",
    "\n",
    "# Assuming 'engine' is already created with SQLAlchemy create_engine function\n",
    "with engine.begin() as connection:  # using begin() to start a transaction context\n",
    "    if inspector.has_table('members_senate'):\n",
    "        for index, row in df.iterrows():\n",
    "            # Check if the member exists in the database\n",
    "            try:\n",
    "                if not member_exists(row['Member_ID'], connection):\n",
    "                    # Prepare the row for insertion\n",
    "                    row_df = pd.DataFrame([row])\n",
    "                    # Exclude the member_titles_Member_ID field if it exists in the DataFrame\n",
    "                    row_df = row_df.drop(columns=['member_titles_Member_ID'], errors='ignore')\n",
    "                    # Insert the member row into the database\n",
    "                    row_df.to_sql('members_senate', con=engine, if_exists='append', index=False)\n",
    "            except IntegrityError as e:\n",
    "                print(f\"IntegrityError encountered: {e.orig.args}\")\n",
    "                print(f\"Skipping duplicate entry for member ID: {row['Member_ID']}\")\n",
    "    else:\n",
    "        # If the table doesn't exist, create and insert rows\n",
    "        df.to_sql('members_senate', con=engine, if_exists='fail', index=False)\n",
    "\n",
    "print(\"09. Scraped current Senate members. Page scrape.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Webpage data scrape for House and Senate Leadership Titles for current session.\n",
    "#\n",
    "\n",
    "def scrape_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    anchor_tags = soup.find_all('a', class_='list-group-item list-group-item-action')\n",
    "    data = []\n",
    "\n",
    "    for tag in anchor_tags:\n",
    "        biography_link = tag.attrs.get('href', '')\n",
    "        matches = re.findall(r'/(.)/(\\d+)', biography_link)\n",
    "        identifier = matches[0][0] + \"M\" + matches[0][1] if matches else ''\n",
    "\n",
    "        cols = tag.find_all('div', class_=['col-sm-6 col-lg-3', 'col-sm-6 col-lg-9 text-right'])\n",
    "        if len(cols) == 2:\n",
    "            leadership_title = cols[0].get_text(strip=True)\n",
    "            if identifier == '' and leadership_title == 'President':\n",
    "                identifier = 'S_Pres'\n",
    "            data.append([identifier, leadership_title])\n",
    "\n",
    "    return pd.DataFrame(data, columns=['Member_ID', 'Leadership_Title'])\n",
    "\n",
    "# URLs to scrape\n",
    "urls = ['https://www.ncleg.gov/Members/Leadership/H', 'https://www.ncleg.gov/Members/Leadership/S']\n",
    "\n",
    "# Create a SQLAlchemy connection\n",
    "DATABASE_URI = \"mysql+pymysql://kurt712:Gwyn-072022!@localhost/NCLeg\"\n",
    "engine = create_engine(DATABASE_URI, echo=False)\n",
    "\n",
    "# Scrape both URLs and combine data\n",
    "dfs = [scrape_url(url) for url in urls]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Check if the table exists using Inspector\n",
    "if 'member_titles' in inspector.get_table_names():\n",
    "    # Fetch the current table into a dataframe\n",
    "    existing_df = pd.read_sql(\"SELECT * FROM member_titles\", engine)\n",
    "    \n",
    "    # Merge the existing dataframe with the new one to detect changes\n",
    "    merged_df = existing_df.merge(combined_df, on=['Member_ID', 'Leadership_Title'], how='outer', indicator=True)\n",
    "    changes = merged_df[merged_df['_merge'] != 'both']\n",
    "    \n",
    "    # If changes exist, print them and update the table\n",
    "    if not changes.empty:\n",
    "        print(changes)\n",
    "        combined_df.to_sql('member_titles', engine, if_exists='replace', index=False)\n",
    "else:\n",
    "    # If the table doesn't exist, create it\n",
    "    combined_df.to_sql('member_titles', engine, if_exists='fail', index=False)\n",
    "\n",
    "print(\"10. Scraped current House and Senate Leadership Titles for current session. Page scrape.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Webpage data scrape for House and Senate Contact Info for current session.\n",
    "#\n",
    "\n",
    "def scrape_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find(\"table\", attrs={\"id\": \"contactTable\"})\n",
    "    rows = table.find_all(\"tr\")[1:] if table else []  # Extract rows if table is found, else return empty list\n",
    "    \n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')  \n",
    "        \n",
    "        # Assuming your table structure matches the expected columns (adjust as needed)\n",
    "        member_url = cols[0].find('a')['href'] if cols[4].find('a') else None\n",
    "        # Extracting the unique identifier\n",
    "        if member_url:\n",
    "            matches = re.findall(r'/(.)/(\\d+)', member_url)\n",
    "            unique_id = matches[0][0] + \"M\" + matches[0][1] if matches else str(uuid.uuid4())\n",
    "        else:\n",
    "            unique_id = str(uuid.uuid4())\n",
    "\n",
    "        data.append({\n",
    "            \"Member_ID\": unique_id,\n",
    "            \"Member_Room\": cols[1].get_text(),\n",
    "            \"Member_Phone\": cols[3].get_text(),\n",
    "            \"Member_Email\": cols[4].get_text()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# URLs to scrape\n",
    "urls = ['https://www.ncleg.gov/Members/ContactInfo/H', 'https://www.ncleg.gov/Members/ContactInfo/S']\n",
    "\n",
    "Base = declarative_base()\n",
    "class MembersContact(Base):\n",
    "    \n",
    "    __tablename__ = 'members_contact_info'\n",
    "\n",
    "    Member_ID = Column(String(25), primary_key=True)\n",
    "    Member_Room = Column(String(20))\n",
    "    Member_Phone = Column(String(15))\n",
    "    Member_Email = Column(String(60))\n",
    "\n",
    "members_contact_info = []\n",
    "\n",
    "# Create a SQLAlchemy connection\n",
    "DATABASE_URI = \"mysql+pymysql://kurt712:Gwyn-072022!@localhost/NCLeg\"\n",
    "engine = create_engine(DATABASE_URI, echo=False)\n",
    "\n",
    "# Scrape both URLs and convert to DataFrames\n",
    "dfs = [scrape_url(url) for url in urls]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Check if the table exists using Inspector\n",
    "if 'member_contact_info' in inspector.get_table_names():\n",
    "    # Fetch the current table into a dataframe\n",
    "    existing_df = pd.read_sql(\"SELECT * FROM member_contact_info\", engine)\n",
    "    \n",
    "    # Merge the existing dataframe with the new one to detect changes\n",
    "    merged_df = existing_df.merge(combined_df, on=['Member_ID', 'Member_Room', 'Member_Phone', 'Member_Email'], how='outer', indicator=True)\n",
    "    changes = merged_df[merged_df['_merge'] != 'both']\n",
    "    \n",
    "    # If changes exist, print them and update the table\n",
    "    if not changes.empty:\n",
    "        print(changes)\n",
    "        combined_df.to_sql('member_contact_info', engine, if_exists='replace', index=False)\n",
    "else:\n",
    "    # If the table doesn't exist, create it\n",
    "    combined_df.to_sql('member_contact_info', engine, if_exists='fail', index=False)\n",
    "\n",
    "print(\"11. Scraped current House and Senate Contact Info for current session. Page scrape.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\" \")\n",
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85e95b-6a8f-403c-a3c6-a2090d967901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
